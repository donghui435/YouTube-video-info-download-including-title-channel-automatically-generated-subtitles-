last video laid structure neural network ill give quick recap fresh mind sand two main goal video first introduce idea gradient descent underlie neural learn lot machine learning works well going dig little particular network perform sand hidden neuron end actually looking reminder goal classic example hand written digit recognition hello world neural pix el grid pix el gray scale value determine activation neuron input layer network activation neuron following based weighted sum activation previous layer plus special number cal led bias compose sum function like sigmoid ora rel u way last video total given somewhat arbitrary choice two hidden neuron network weight bias es adjust value determine exactly network know actually mean say network given digit neuron final layer correspond digit remember motivation mind layered structure maybe second layer could pick edge third layer might pick pattern like loop line sand last one could piece together pattern recognize learn network want algorithm show network whole bunch training data comes form bunch different image hand written along theyre supposed adjust weight bias es improve performance training data hopefully layered structure mean image beyond training data way test train network show theta never seen see accurately new us make common example start good people behind base put together collection thousand hand written digit image one theyre supposed provocative describe machine learning actually see lot less like crazy premise lot like well calculus exercise mean basically comes finding minimum certain function remember conceptually thinking neuron connected neuron previous layer weight weighted sum de fining activation kind like connection sand bias indication whether neuron tend active inactive start thing gon na initialize weight bias es totally randomly needless say network going perform pretty horribly given training example since something random example feed image output layer look like mess define cost function way telling computer bad computer output activation zero neuron one neuron gave utter trash say little mathematically add square difference trash output activation value want well call cost single training example notice sum small network confidently image correctly large network seem like doesnt really know consider average cost thousand training example disposal average cost measure lousy network bad computer feel thats complicated thing remember network basically function one take input pix el value spit ten output sense weight cost function layer complexity top take input thirteen thousand weight bias es spit single number de scribing bad weight bias es way defined depend behavior thousand piece training data thats lot think telling computer job helpful want tell change weight bias es get better make easier rather struggling imagine function imagine simple function one number input one number output find input minimize value function calculus student know sometimes figure minimum explicitly thats always feasible really complicated thirteen thousand input version situation crazy complicated neural network cost function flexible tactic start old input figure direction step make output lower specifically figure slope function shift left slope positive shift input right slope negative repeatedly point check ing new slope taking appropriate gon na approach local minimum function image might mind ball rolling hill notice even really simplified single input function many possible valley might land random input start theres guarantee local minimum land going possible value cost function thats going carry neural network case well also want notice make step sizes proportional slope slope flattening towards minimum step get smaller smaller kind help complexity bit imagine instead function two input one output might think input space x plane cost function surface instead slope function ask direction step input space decrease output function quickly word whats downhill direction helpful think ball rolling hill familiar calculus know gradient function give direction ascent basically direction step increase function quickly naturally enough taking negative gradient give direction step decrease function quickly even length gradient vector actually indication steep slope unfamiliar calculus want learn check work khan academy topic honestly though matter right principle exist way compute vector vector tell downhill direction steep thats know rock solid get algorithm function compute gradient direction take small step downhill repeat basic idea function input instead two input imagine weight bias es network giant column vector negative gradient cost function vector direction inside insanely huge input space tell going cause rapid decrease cost function course specially designed cost weight bias es decrease mean making output network piece training data look less like random array ten value like actual decision want make important remember cost function involve average training data minimize mean better performance algorithm gradient efficiently effectively heart neural network learn cal led back propagation going talking next video really want take time walk exactly happen weight bias given piece training data trying give intuitive feel whats happening beyond pile relevant calculus right main thing want know independent implementation detail sis mean talk network learning cost function notice one consequence important cost function nice smooth output find local minimum taking little step downhill way artificial neuron continuously ranging activation rather simply active inactive binary way way biological neuron process repeatedly input function multiple negative gradient cal led gradient descent way converge towards local minimum cost function basically valley still showing picture function two input course nudge thirteen thousand dimensional input space little hard wrap mind around actually nice non spatial way think component negative gradient tell us two thing sign course tell us whether corresponding component input vector nudge importantly relative magnitude tell matter see network adjustment one weight might much greater impact cost function adjustment connection matter training data way think gradient vector cost function encode relative importance weight bias going carry bang buck really another way thinking direction take simpler example function two variable input compute gradient particular point comes one hand interpret saying standing input moving along direction increase function quickly graph function plane input point vector whats giving straight uphill direction another way read say first variable three times importance second variable least neighborhood relevant x value lot bang buck zoom sum far network function input output defined term weighted cost function layer complexity top take weight bias es input spit single measure lousy ness based training example gradient cost function one layer complexity still tell us nudge weight bias es cause change value cost function might interpret saying weight matter initialize network random weight bias es adjust many times based gradient descent process well actually perform image never seen well one describe two hidden sixteen neuron chosen mostly aesthetic reason swell bad percent new image correctly honestly look example kind feel compel led cut little slack play around hidden layer structure make couple get thats pretty good best certainly get better performance getting sophisticated plain vanilla network given daunting initial task think theres something incredible network well image never seen given never specifically told pattern look originally way motivate structure de scribing hope might second layer might pick little third layer would piece together edge recognize loop longer might piece together recognize network actually well one least remember last video loo ked weight neuron first layer given neuron second layer visualize given pix el pattern second layer neuron well actually weight associated transition first layer next instead isolated little edge look well almost random put loose pattern middle would seem unfathomably dimensional space possible weight bias es network found happy little local minimum despite successfully image doesnt exactly pick pattern might hoped really drive point home watch happen input random image system smart might expect either feel uncertain maybe really output neuron evenly instead confidently give nonsense answer sure random noise actual image differently even network recognize pretty well idea draw tightly constrained training setup mean put point view entire universe consist nothing clearly defined unmoving centered tiny grid cost function never gave incentive anything utterly confident image second layer neuron really might wonder would introduce network motivation edge pattern si mean thats end well meant end goal instead starting point frankly old technology kind need understand understand detailed modern variant clearly capable interesting dig hidden really less intelligent focus moment learn happen engage actively material somehow one pretty simple thing want pause right think deeply moment might make system perceive image wan ted better pick thing like edge better actually engage recommend book deep learning neural network sin find code data load play exact example book walk step step code whats awesome book free publicly available get something consider joining making donation towards also linked couple resource like lot description phenomenal beautiful post la article distill close thing last minute si want jump back snippet interview lei sha lee might remember last video work deep learning little snippet talk two recent really dig modern image recognition actually learning set conversation first paper took one particularly deep neural really good image recognition instead training properly data set around training obviously testing accuracy going better random since everything randomly still able achieve training accuracy would properly millions weight particular network enough memorize random data kind question whether cost function actually correspond sort structure image know memorize entire data set correct classification couple know half year later exactly rebuttal paper paper ad dressed ked like hey actually something little bit smart er look accuracy curve training random data set curve sort went know slowly almost kind linear fashion really struggling find local minima possible know right weight would get accuracy whereas actually training structured data set one right know fiddle around little bit beginning kind drop ped fast get accuracy level sense easier find local maxima also interesting caught bring light another paper actually couple year ago lot network one result saying look optimization landscape local minima tend learn actually equal quality sense data set structure able find much easily thanks always supporting said game changer video really would possible without also want give special thanks v c firm partner support initial video series focus early stage machine learning ai feel pretty confident watching even likely people know right early getting company ground folk would love hear founder sand even set address video reach three blue one brown amplify partner c om